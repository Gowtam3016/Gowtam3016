{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQoPZcdAOgBc"
      },
      "outputs": [],
      "source": [
        "# RETAIL SALES FORECASTING: A COMPREHENSIVE CASE STUDY\n",
        "# ===================================================\n",
        "# This case study demonstrates a complete machine learning pipeline for forecasting\n",
        "# daily sales for a retail chain across multiple stores and product categories.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as XGBRegressor\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import warnings\n",
        "import joblib\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. DATA GENERATION\n",
        "# -----------------\n",
        "# In a real case study, you would load your data from files/database\n",
        "# For this example, we'll generate realistic retail sales data\n",
        "\n",
        "def generate_retail_sales_data(start_date='2020-01-01', periods=730):\n",
        "    \"\"\"Generate synthetic retail sales data with multiple stores and categories.\"\"\"\n",
        "    dates = pd.date_range(start=start_date, periods=periods)\n",
        "    stores = ['Store_' + str(i) for i in range(1, 6)]  # 5 stores\n",
        "    categories = ['Electronics', 'Clothing', 'Groceries', 'Home', 'Beauty']\n",
        "\n",
        "    # Create empty dataframe\n",
        "    data = []\n",
        "\n",
        "    for store in stores:\n",
        "        for category in categories:\n",
        "            # Base level varies by store and category\n",
        "            base_level = np.random.randint(5000, 15000)\n",
        "\n",
        "            # Category-specific trend\n",
        "            if category == 'Electronics':\n",
        "                trend_factor = 20  # Growing\n",
        "            elif category == 'Clothing':\n",
        "                trend_factor = 10  # Moderate growth\n",
        "            elif category == 'Groceries':\n",
        "                trend_factor = 5   # Stable growth\n",
        "            else:\n",
        "                trend_factor = 0   # Neutral\n",
        "\n",
        "            # Store-specific performance\n",
        "            if 'Store_1' in store:\n",
        "                store_factor = 1.2  # Best performing\n",
        "            elif 'Store_5' in store:\n",
        "                store_factor = 0.8  # Worst performing\n",
        "            else:\n",
        "                store_factor = 1.0  # Average\n",
        "\n",
        "            # Generate daily sales with various patterns\n",
        "            for i, date in enumerate(dates):\n",
        "                # Base value\n",
        "                sales = base_level * store_factor\n",
        "\n",
        "                # Add trend component\n",
        "                sales += i * trend_factor\n",
        "\n",
        "                # Add weekly seasonality (weekends have higher sales)\n",
        "                if date.dayofweek >= 5:  # Weekend\n",
        "                    sales *= 1.3\n",
        "\n",
        "                # Add monthly seasonality (higher sales at month start)\n",
        "                if date.day <= 5:\n",
        "                    sales *= 1.1\n",
        "\n",
        "                # Add quarterly seasonality (Q4 has higher sales - holiday season)\n",
        "                if date.quarter == 4:\n",
        "                    sales *= 1.4\n",
        "\n",
        "                # Add yearly seasonality\n",
        "                if date.month in [11, 12]:  # Holiday season\n",
        "                    sales *= 1.5\n",
        "                elif date.month in [1, 2]:  # Post-holiday slump\n",
        "                    sales *= 0.8\n",
        "\n",
        "                # Add promotions (random 20% boost on some days)\n",
        "                promo = np.random.choice([0, 1], p=[0.9, 0.1])\n",
        "                if promo:\n",
        "                    sales *= 1.2\n",
        "\n",
        "                # Add weather effect (sales drop on \"rainy\" days)\n",
        "                weather_bad = np.random.choice([0, 1], p=[0.85, 0.15])\n",
        "                if weather_bad:\n",
        "                    sales *= 0.9\n",
        "\n",
        "                # Add noise\n",
        "                sales *= np.random.normal(1, 0.05)\n",
        "\n",
        "                # Record the data\n",
        "                data.append({\n",
        "                    'date': date,\n",
        "                    'store': store,\n",
        "                    'category': category,\n",
        "                    'sales': max(0, round(sales, 2)),\n",
        "                    'promo': promo,\n",
        "                    'bad_weather': weather_bad\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Add some store-specific attributes\n",
        "    store_attributes = pd.DataFrame({\n",
        "        'store': stores,\n",
        "        'size_sqm': [5000, 3500, 7000, 4500, 2500],\n",
        "        'years_open': [10, 5, 7, 3, 2],\n",
        "        'city': ['New York', 'Chicago', 'Los Angeles', 'Houston', 'Phoenix']\n",
        "    })\n",
        "\n",
        "    return df, store_attributes\n",
        "\n",
        "# Generate our dataset\n",
        "sales_df, store_info = generate_retail_sales_data(periods=730)  # 2 years of data\n",
        "\n",
        "print(\"Sample of generated sales data:\")\n",
        "print(sales_df.head())\n",
        "print(\"\\nStore information:\")\n",
        "print(store_info)\n",
        "\n",
        "# 2. EXPLORATORY DATA ANALYSIS\n",
        "# ---------------------------\n",
        "\n",
        "print(\"\\n== EXPLORATORY DATA ANALYSIS ==\")\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(sales_df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(sales_df.isnull().sum())\n",
        "\n",
        "# Merge store information\n",
        "sales_df = sales_df.merge(store_info, on='store', how='left')\n",
        "\n",
        "# Analyze sales by store\n",
        "store_sales = sales_df.groupby('store')['sales'].agg(['mean', 'median', 'std', 'sum'])\n",
        "print(\"\\nSales by store:\")\n",
        "print(store_sales)\n",
        "\n",
        "# Analyze sales by category\n",
        "category_sales = sales_df.groupby('category')['sales'].agg(['mean', 'median', 'std', 'sum'])\n",
        "print(\"\\nSales by category:\")\n",
        "print(category_sales)\n",
        "\n",
        "# Time-based analysis\n",
        "sales_df['year'] = sales_df['date'].dt.year\n",
        "sales_df['month'] = sales_df['date'].dt.month\n",
        "sales_df['day_of_week'] = sales_df['date'].dt.dayofweek\n",
        "sales_df['day_of_month'] = sales_df['date'].dt.day\n",
        "sales_df['is_weekend'] = sales_df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "sales_df['quarter'] = sales_df['date'].dt.quarter\n",
        "\n",
        "# Monthly sales trend\n",
        "monthly_sales = sales_df.groupby(['year', 'month'])['sales'].sum().reset_index()\n",
        "print(\"\\nMonthly sales trend:\")\n",
        "print(monthly_sales.head())\n",
        "\n",
        "# Day of week analysis\n",
        "dow_sales = sales_df.groupby('day_of_week')['sales'].mean().reset_index()\n",
        "print(\"\\nAverage sales by day of week:\")\n",
        "print(dow_sales)\n",
        "\n",
        "# Effect of promotions\n",
        "promo_effect = sales_df.groupby('promo')['sales'].agg(['mean', 'median', 'count'])\n",
        "print(\"\\nImpact of promotions:\")\n",
        "print(promo_effect)\n",
        "\n",
        "# Effect of bad weather\n",
        "weather_effect = sales_df.groupby('bad_weather')['sales'].agg(['mean', 'median', 'count'])\n",
        "print(\"\\nImpact of bad weather:\")\n",
        "print(weather_effect)\n",
        "\n",
        "# 3. VISUALIZATIONS\n",
        "# ----------------\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Overall sales trend\n",
        "plt.subplot(2, 2, 1)\n",
        "sales_ts = sales_df.groupby('date')['sales'].sum()\n",
        "sales_ts.plot(title='Daily Total Sales', ax=plt.gca())\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Sales by store\n",
        "plt.subplot(2, 2, 2)\n",
        "store_totals = sales_df.groupby('store')['sales'].sum()\n",
        "store_totals.plot(kind='bar', title='Total Sales by Store', ax=plt.gca())\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 3: Sales by category\n",
        "plt.subplot(2, 2, 3)\n",
        "cat_totals = sales_df.groupby('category')['sales'].sum()\n",
        "cat_totals.plot(kind='bar', title='Total Sales by Category', ax=plt.gca())\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 4: Avg sales by day of week\n",
        "plt.subplot(2, 2, 4)\n",
        "dow_sales.plot(kind='bar', x='day_of_week', y='sales',\n",
        "               title='Average Sales by Day of Week', ax=plt.gca())\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('eda_plots.png')\n",
        "\n",
        "# Time series decomposition for a specific store/category\n",
        "single_series = sales_df[(sales_df['store'] == 'Store_1') &\n",
        "                         (sales_df['category'] == 'Electronics')]\n",
        "daily_sales = single_series.set_index('date')['sales']\n",
        "\n",
        "# Perform seasonal decomposition\n",
        "decomposition = seasonal_decompose(daily_sales, model='additive', period=7)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "decomposition.plot()\n",
        "plt.tight_layout()\n",
        "plt.savefig('seasonal_decomposition.png')\n",
        "\n",
        "# 4. FEATURE ENGINEERING\n",
        "# ---------------------\n",
        "\n",
        "print(\"\\n== FEATURE ENGINEERING ==\")\n",
        "\n",
        "# Create features for forecasting\n",
        "def create_features(df):\n",
        "    \"\"\"Extract time-based features from date.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Calendar features\n",
        "    df['dayofweek'] = df['date'].dt.dayofweek\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['dayofyear'] = df['date'].dt.dayofyear\n",
        "    df['dayofmonth'] = df['date'].dt.day\n",
        "    df['weekofyear'] = df['date'].dt.isocalendar().week\n",
        "\n",
        "    # Flag weekends and holidays\n",
        "    df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "    # Add holiday flags (simplified)\n",
        "    df['is_holiday'] = ((df['month'] == 12) & (df['dayofmonth'] == 25)) | \\\n",
        "                       ((df['month'] == 1) & (df['dayofmonth'] == 1)) | \\\n",
        "                       ((df['month'] == 7) & (df['dayofmonth'] == 4))\n",
        "\n",
        "    # Cyclical features for day of week, month, etc.\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
        "    df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)\n",
        "    df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)\n",
        "\n",
        "    # Moving averages (lag features)\n",
        "    for store in df['store'].unique():\n",
        "        for category in df['category'].unique():\n",
        "            mask = (df['store'] == store) & (df['category'] == category)\n",
        "            df.loc[mask, 'sales_lag7'] = df.loc[mask, 'sales'].shift(7)\n",
        "            df.loc[mask, 'sales_lag14'] = df.loc[mask, 'sales'].shift(14)\n",
        "            df.loc[mask, 'sales_lag28'] = df.loc[mask, 'sales'].shift(28)\n",
        "            df.loc[mask, 'sales_mean7'] = df.loc[mask, 'sales'].shift().rolling(window=7).mean()\n",
        "            df.loc[mask, 'sales_mean28'] = df.loc[mask, 'sales'].shift().rolling(window=28).mean()\n",
        "\n",
        "    # Drop rows with NaN (lag features will create NaN for first rows)\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "model_df = create_features(sales_df)\n",
        "\n",
        "# Check the new features\n",
        "print(\"Dataframe with new features:\")\n",
        "print(model_df.columns.tolist())\n",
        "print(model_df.head())\n",
        "\n",
        "# 5. PREPARING FOR MODELING\n",
        "# ------------------------\n",
        "\n",
        "# Split data temporally: use first 80% for training, next 10% for validation, last 10% for test\n",
        "model_df = model_df.sort_values('date')\n",
        "train_cutoff = int(len(model_df) * 0.8)\n",
        "val_cutoff = int(len(model_df) * 0.9)\n",
        "\n",
        "train_df = model_df.iloc[:train_cutoff]\n",
        "val_df = model_df.iloc[train_cutoff:val_cutoff]\n",
        "test_df = model_df.iloc[val_cutoff:]\n",
        "\n",
        "print(f\"\\nData splits: Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "# Define features and target\n",
        "cat_features = ['store', 'category', 'city']\n",
        "num_features = ['size_sqm', 'years_open', 'promo', 'bad_weather',\n",
        "                'dayofweek', 'month', 'year', 'is_weekend', 'is_holiday',\n",
        "                'month_sin', 'month_cos', 'dow_sin', 'dow_cos',\n",
        "                'sales_lag7', 'sales_lag14', 'sales_lag28',\n",
        "                'sales_mean7', 'sales_mean28']\n",
        "\n",
        "features = cat_features + num_features\n",
        "target = 'sales'\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "    ])\n",
        "\n",
        "# 6. MODEL TRAINING AND EVALUATION\n",
        "# -------------------------------\n",
        "\n",
        "print(\"\\n== MODEL TRAINING ==\")\n",
        "\n",
        "# Define models to try\n",
        "models = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
        "    'XGBoost': XGBRegressor.XGBRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "\n",
        "    # Create pipeline with preprocessing\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    # Fit the model\n",
        "    pipeline.fit(train_df[features], train_df[target])\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_preds = pipeline.predict(val_df[features])\n",
        "    val_mae = mean_absolute_error(val_df[target], val_preds)\n",
        "    val_rmse = np.sqrt(mean_squared_error(val_df[target], val_preds))\n",
        "    val_r2 = r2_score(val_df[target], val_preds)\n",
        "\n",
        "    print(f\"  Validation - MAE: {val_mae:.2f}, RMSE: {val_rmse:.2f}, R²: {val_r2:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'model': pipeline,\n",
        "        'val_mae': val_mae,\n",
        "        'val_rmse': val_rmse,\n",
        "        'val_r2': val_r2\n",
        "    }\n",
        "\n",
        "# Find the best model\n",
        "best_model_name = min(results, key=lambda x: results[x]['val_rmse'])\n",
        "best_model = results[best_model_name]['model']\n",
        "\n",
        "print(f\"\\nBest model: {best_model_name} with RMSE: {results[best_model_name]['val_rmse']:.2f}\")\n",
        "\n",
        "# 7. HYPERPARAMETER TUNING\n",
        "# -----------------------\n",
        "\n",
        "print(\"\\n== HYPERPARAMETER TUNING ==\")\n",
        "\n",
        "# Focus on tuning the best model\n",
        "if best_model_name == 'RandomForest':\n",
        "    print(\"Tuning RandomForest...\")\n",
        "\n",
        "    param_grid = {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__max_depth': [None, 10, 20, 30],\n",
        "        'model__min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "\n",
        "elif best_model_name == 'GradientBoosting':\n",
        "    print(\"Tuning GradientBoosting...\")\n",
        "\n",
        "    param_grid = {\n",
        "        'model__n_estimators': [100, 200],\n",
        "        'model__learning_rate': [0.01, 0.1],\n",
        "        'model__max_depth': [3, 5, 7]\n",
        "    }\n",
        "\n",
        "elif best_model_name == 'XGBoost':\n",
        "    print(\"Tuning XGBoost...\")\n",
        "\n",
        "    param_grid = {\n",
        "        'model__n_estimators': [100, 200],\n",
        "        'model__learning_rate': [0.01, 0.1],\n",
        "        'model__max_depth': [3, 5, 7]\n",
        "    }\n",
        "\n",
        "else:  # LinearRegression\n",
        "    print(\"Linear Regression doesn't require hyper-parameter tuning. Moving on...\")\n",
        "    param_grid = {}\n",
        "\n",
        "if param_grid:\n",
        "    # Create TimeSeriesSplit for time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "    # Setup GridSearchCV\n",
        "    grid_search = GridSearchCV(\n",
        "        best_model,\n",
        "        param_grid,\n",
        "        cv=tscv,\n",
        "        scoring='neg_root_mean_squared_error',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Fit grid search\n",
        "    grid_search.fit(train_df[features], train_df[target])\n",
        "\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {-grid_search.best_score_:.2f} RMSE\")\n",
        "\n",
        "    # Update best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "# 8. FINAL MODEL EVALUATION\n",
        "# -----------------------\n",
        "\n",
        "print(\"\\n== FINAL MODEL EVALUATION ==\")\n",
        "\n",
        "# Make predictions on test set\n",
        "test_preds = best_model.predict(test_df[features])\n",
        "\n",
        "# Calculate metrics\n",
        "test_mae = mean_absolute_error(test_df[target], test_preds)\n",
        "test_rmse = np.sqrt(mean_squared_error(test_df[target], test_preds))\n",
        "test_r2 = r2_score(test_df[target], test_preds)\n",
        "\n",
        "print(f\"Test - MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, R²: {test_r2:.4f}\")\n",
        "\n",
        "# Plot actual vs predicted for a single store/category\n",
        "test_df['predicted'] = test_preds\n",
        "\n",
        "single_test = test_df[(test_df['store'] == 'Store_1') &\n",
        "                     (test_df['category'] == 'Electronics')]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(single_test['date'], single_test['sales'], label='Actual')\n",
        "plt.plot(single_test['date'], single_test['predicted'], label='Predicted')\n",
        "plt.title('Actual vs Predicted Sales: Store_1, Electronics')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('actual_vs_predicted.png')\n",
        "\n",
        "# 9. FEATURE IMPORTANCE ANALYSIS\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n== FEATURE IMPORTANCE ANALYSIS ==\")\n",
        "\n",
        "if hasattr(best_model[-1], 'feature_importances_'):\n",
        "    # Get feature names from the preprocessor\n",
        "    cat_features_transformed = best_model[0].transformers_[1][1].get_feature_names_out(cat_features)\n",
        "    feature_names = np.array(num_features + cat_features_transformed.tolist())\n",
        "\n",
        "    # Get feature importances\n",
        "    importances = best_model[-1].feature_importances_\n",
        "\n",
        "    # Sort by importance\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    # Print top 20 features\n",
        "    print(\"\\nTop 20 most important features:\")\n",
        "    for i in range(min(20, len(feature_names))):\n",
        "        print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
        "\n",
        "    # Plot feature importances\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.title('Feature Importances')\n",
        "    plt.bar(range(min(20, len(feature_names))),\n",
        "            importances[indices[:20]],\n",
        "            align='center')\n",
        "    plt.xticks(range(min(20, len(feature_names))),\n",
        "               feature_names[indices[:20]], rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importances.png')\n",
        "\n",
        "# 10. FUTURE SALES FORECASTING\n",
        "# ---------------------------\n",
        "\n",
        "print(\"\\n== FUTURE SALES FORECASTING ==\")\n",
        "\n",
        "def forecast_future_sales(model, last_data, features, n_days=30):\n",
        "    \"\"\"Forecast sales for n days into the future for all store/category combinations.\"\"\"\n",
        "    # Get unique stores and categories\n",
        "    stores = last_data['store'].unique()\n",
        "    categories = last_data['category'].unique()\n",
        "\n",
        "    # Get the last date in our data\n",
        "    last_date = last_data['date'].max()\n",
        "\n",
        "    # Create a dataframe for future dates\n",
        "    future_dates = [last_date + timedelta(days=i+1) for i in range(n_days)]\n",
        "\n",
        "    # Initialize list to store forecasts\n",
        "    forecasts = []\n",
        "\n",
        "    # For each store and category\n",
        "    for store in stores:\n",
        "        for category in categories:\n",
        "            # Get store attributes\n",
        "            store_attrs = last_data[last_data['store'] == store][['size_sqm', 'years_open', 'city']].iloc[0].to_dict()\n",
        "\n",
        "            # Filter data for this store/category\n",
        "            store_cat_data = last_data[(last_data['store'] == store) &\n",
        "                                      (last_data['category'] == category)].sort_values('date')\n",
        "\n",
        "            # For each future date\n",
        "            for future_date in future_dates:\n",
        "                # Create a row for the future date\n",
        "                future_row = {\n",
        "                    'date': future_date,\n",
        "                    'store': store,\n",
        "                    'category': category,\n",
        "                    'promo': np.random.choice([0, 1], p=[0.9, 0.1]),  # Random promotion\n",
        "                    'bad_weather': np.random.choice([0, 1], p=[0.85, 0.15]),  # Random weather\n",
        "                    'size_sqm': store_attrs['size_sqm'],\n",
        "                    'years_open': store_attrs['years_open'],\n",
        "                    'city': store_attrs['city'],\n",
        "                    'dayofweek': future_date.dayofweek,\n",
        "                    'month': future_date.month,\n",
        "                    'year': future_date.year,\n",
        "                    'dayofyear': future_date.timetuple().tm_yday,\n",
        "                    'dayofmonth': future_date.day,\n",
        "                    'weekofyear': future_date.isocalendar()[1],\n",
        "                    'is_weekend': 1 if future_date.dayofweek >= 5 else 0,\n",
        "                    'is_holiday': 1 if ((future_date.month == 12 and future_date.day == 25) or\n",
        "                                        (future_date.month == 1 and future_date.day == 1) or\n",
        "                                        (future_date.month == 7 and future_date.day == 4)) else 0,\n",
        "                    'month_sin': np.sin(2 * np.pi * future_date.month/12),\n",
        "                    'month_cos': np.cos(2 * np.pi * future_date.month/12),\n",
        "                    'dow_sin': np.sin(2 * np.pi * future_date.dayofweek/7),\n",
        "                    'dow_cos': np.cos(2 * np.pi * future_date.dayofweek/7)\n",
        "                }\n",
        "\n",
        "                # Calculate lag features\n",
        "                # We need to get recent sales for this store/category\n",
        "                recent_sales = store_cat_data['sales'].tolist()\n",
        "\n",
        "                if len(recent_sales) >= 28:\n",
        "                    future_row['sales_lag7'] = recent_sales[-7]\n",
        "                    future_row['sales_lag14'] = recent_sales[-14]\n",
        "                    future_row['sales_lag28'] = recent_sales[-28]\n",
        "                    future_row['sales_mean7'] = np.mean(recent_sales[-7:])\n",
        "                    future_row['sales_mean28'] = np.mean(recent_sales[-28:])\n",
        "                else:\n",
        "                    # Not enough history, use what we have or default values\n",
        "                    future_row['sales_lag7'] = recent_sales[-min(7, len(recent_sales))] if recent_sales else 0\n",
        "                    future_row['sales_lag14'] = recent_sales[-min(14, len(recent_sales))] if recent_sales else 0\n",
        "                    future_row['sales_lag28'] = recent_sales[-min(28, len(recent_sales))] if recent_sales else 0\n",
        "                    future_row['sales_mean7'] = np.mean(recent_sales[-min(7, len(recent_sales)):]) if recent_sales else 0\n",
        "                    future_row['sales_mean28'] = np.mean(recent_sales[-min(28, len(recent_sales)):]) if recent_sales else 0\n",
        "\n",
        "                forecasts.append(future_row)\n",
        "\n",
        "    # Convert to dataframe\n",
        "    forecast_df = pd.DataFrame(forecasts)\n",
        "\n",
        "    # Make predictions\n",
        "    forecast_df['predicted_sales'] = model.predict(forecast_df[features])\n",
        "\n",
        "    return forecast_df\n",
        "\n",
        "# Generate forecasts for the next 30 days\n",
        "forecast_df = forecast_future_sales(best_model, test_df, features, n_days=30)\n",
        "\n",
        "# Show example forecasts\n",
        "print(\"\\nSample forecasts for next 30 days:\")\n",
        "print(forecast_df[['date', 'store', 'category', 'predicted_sales']].head(15))\n",
        "\n",
        "# Plot forecasts for a specific store/category\n",
        "single_forecast = forecast_df[(forecast_df['store'] == 'Store_1') &\n",
        "                             (forecast_df['category'] == 'Electronics')]\n",
        "\n",
        "# Combine historical and forecast data for plotting\n",
        "hist_data = test_df[(test_df['store'] == 'Store_1') &\n",
        "                   (test_df['category'] == 'Electronics')]\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(hist_data['date'], hist_data['sales'], label='Historical', color='blue')\n",
        "plt.plot(single_forecast['date'], single_forecast['predicted_sales'],\n",
        "         label='Forecast', color='red', linestyle='--')\n",
        "plt.title('Sales Forecast: Store_1, Electronics')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('forecast.png')\n",
        "\n",
        "# 11. MODEL DEPLOYMENT\n",
        "# ------------------\n",
        "\n",
        "print(\"\\n== MODEL DEPLOYMENT ==\")\n",
        "\n",
        "# Save the model\n",
        "model_filename = 'retail_sales_forecast_model.joblib'\n",
        "joblib.dump(best_model, model_filename)\n",
        "print(f\"Model saved to {model_filename}\")\n",
        "\n",
        "# Create a simple function for making predictions\n",
        "def predict_sales(model, store, category, date, promo=0, bad_weather=0):\n",
        "    \"\"\"Make a sales prediction for a specific store/category on a specific date.\"\"\"\n",
        "    # Get store info\n",
        "    store_info_row = store_info[store_info['store'] == store].iloc[0]\n",
        "\n",
        "    # Create features\n",
        "    row = {\n",
        "        'date': pd.to_datetime(date),\n",
        "        'store': store,\n",
        "        'category': category,\n",
        "        'promo': promo,\n",
        "        'bad_weather': bad_weather,\n",
        "        'size_sqm': store_info_row['size_sqm'],\n",
        "        'years_open': store_info_row['years_open'],\n",
        "        'city': store_info_row['city']\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame([row])\n",
        "\n",
        "    # Add time features\n",
        "    df['dayofweek'] = df['date'].dt.dayofweek\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['dayofyear'] = df['date'].dt.dayofyear\n",
        "    df['dayofmonth'] = df['date'].dt.day\n",
        "    df['weekofyear'] = df['date'].dt.isocalendar().week\n",
        "    df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "    df['is_holiday'] = ((df['month'] == 12) & (df['dayofmonth'] == 25)) | \\\n",
        "                     ((df['month'] == 1) & (df['dayofmonth'] == 1)) | \\\n",
        "                     ((df['month'] == 7) & (df['dayofmonth'] == 4))\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
        "    df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)\n",
        "    df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)\n",
        "\n",
        "    # Add lag features\n",
        "    # In a real deployment, you would have a database of historical sales to pull from\n",
        "    # For this demo, we'll use placeholder values from the test set\n",
        "    store_cat_data = test_df[(test_df['store'] == store) &\n",
        "                          (test_df['category'] == category)].sort_values('date')\n",
        "\n",
        "    if len(store_cat_data) > 0:\n",
        "        recent_sales = store_cat_data['sales'].tolist()\n",
        "        df['sales_lag7'] = recent_sales[-min(7, len(recent_sales))]\n",
        "        df['sales_lag14'] = recent_sales[-min(14, len(recent_sales))]\n",
        "        df['sales_lag28'] = recent_sales[-min(28, len(recent_sales))]\n",
        "        df['sales_mean7'] = np.mean(recent_sales[-min(7, len(recent_sales)):])\n",
        "        df['sales_mean28'] = np.mean(recent_sales[-min(28, len(recent_sales)):])\n",
        "    else:\n",
        "        # Default values if no history\n",
        "        df['sales_lag7'] = df['sales_lag14'] = df['sales_lag28'] = \\\n",
        "        df['sales_mean7'] = df['sales_mean28'] = 0\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(df[features])[0]\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Demonstrate prediction function\n",
        "future_date = (test_df['date'].max() + timedelta(days=7)).strftime('%Y-%m-%d')\n",
        "prediction = predict_sales(best_model, 'Store_1', 'Electronics', future_date, promo=1)\n",
        "print(f\"\\nPredicted sales for Store_1, Electronics on {future_date} with promotion: ${prediction:.2f}\")\n",
        "\n",
        "# 12. MONITORING AND EVALUATION FRAMEWORK\n",
        "# -------------------------------------\n",
        "\n",
        "print(\"\\n== MONITORING FRAMEWORK ==\")\n",
        "\n",
        "def calculate_deviation(actual, predicted):\n",
        "    \"\"\"Calculate percentage deviation between actual and predicted.\"\"\"\n",
        "    return (actual - predicted) / actual * 100\n",
        "\n",
        "def monitor_model_performance(model, new_data):\n",
        "    \"\"\"Simulate monitoring model performance as new data arrives.\"\"\"\n",
        "    # Make predictions\n",
        "    predictions = model.predict(new_data[features])\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error"
      ]
    }
  ]
}